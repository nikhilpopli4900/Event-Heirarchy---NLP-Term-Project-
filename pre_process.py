# -*- coding: utf-8 -*-
"""NLP_tokenize

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HZprp_QJdTVjKJDoMsEbUz5I9zh_X0AO
"""

# Import the data set

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

!pip install xmltodict

!pip install transformers

import xmltodict
import csv
import torch
from transformers import BertTokenizer, BertModel

with open('/content/drive/MyDrive/hievents/article-10901.xml','r') as f:
  text = xmltodict.parse(f.read())['ArticleInfo']['Text']
  text = text.lower()
  print(text)
  f.close()

# Split the sentence into tokens.
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokenized_text = tokenizer.tokenize(text)

print(tokenized_text)
# for i in range(0,len(tokenized_text),5):
#   print(tokenized_text[i:i+5])

words = list()

for token in tokenized_text:
  if len(token) >= 2 and token[:2] == '##':
    words[-1].append(token)
  else:
    words.append(list())
    words[-1].append(token)

# for word in words:
#   print(word)

word_position = list()
start = 0

for word in words:
  word_position.append(text.find(word[0], start))
  for word_part in word:
    # print(word_part)
    # print(word_part, text.find(word_part, start))
    start = text.find(word_part, start) + len(word_part)

position_to_token_index = {word_position[i]: i for i in range(len(word_position))}

print(position_to_token_index)
# for i in range(len(words)):
#   print(words[i][0], word_position[i])

!pip install transformers

# Commented out IPython magic to ensure Python compatibility.
import torch
from transformers import BertTokenizer, BertModel

# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows
import logging
#logging.basicConfig(level=logging.INFO)

import matplotlib.pyplot as plt
# % matplotlib inline

def get_embedding(text):
    # Load pre-trained model tokenizer (vocabulary)
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    # text = "Here is the sentence I want embeddings for."
    # marked_text = "[CLS] " + text + " [SEP]"

    # # Tokenize our sentence with the BERT tokenizer.
    # tokenized_text = tokenizer.tokenize(marked_text)

    # # Print out the tokens.
    # print (tokenized_text)

    # list(tokenizer.vocab.keys())[5000:5020]

    # Define a new example sentence with multiple meanings of the word "bank"
    #text = "After stealing money from the bank vault, the bank robber was seen " \
           #"fishing on the Mississippi river bank."

    # Add the special tokens.
    marked_text = "[CLS] " + text + " [SEP]"

    # Split the sentence into tokens.
    tokenized_text = tokenizer.tokenize(marked_text)

    # Map the token strings to their vocabulary indeces.
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
    # for tup in zip(tokenized_text, indexed_tokens):
      # print('{:<12} {:>6,}'.format(tup[0], tup[1]))
    # # Display the words with their indeces.
    # for tup in zip(tokenized_text, indexed_tokens):
    #     print('{:<12} {:>6,}'.format(tup[0], tup[1]))

    segments_ids = [1] * len(tokenized_text)

    #print (segments_ids)

    # Convert inputs to PyTorch tensors
    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])

    # Load pre-trained model (weights)
    model = BertModel.from_pretrained('bert-base-uncased',
                                      output_hidden_states = True, # Whether the model returns all hidden-states.
                                      )

    # Put the model in "evaluation" mode, meaning feed-forward operation.
    model.eval()

    with torch.no_grad():

        outputs = model(tokens_tensor, segments_tensors)

        # Evaluating the model will return a different number of objects based on 
        # how it's  configured in the `from_pretrained` call earlier. In this case, 
        # becase we set `output_hidden_states = True`, the third item will be the 
        # hidden states from all layers. See the documentation for more details:
        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel
        hidden_states = outputs[2]

    #print ("Number of layers:", len(hidden_states), "  (initial embeddings + 12 BERT layers)")
    layer_i = 0

    #print ("Number of batches:", len(hidden_states[layer_i]))
    batch_i = 0

    #print ("Number of tokens:", len(hidden_states[layer_i][batch_i]))
    token_i = 0

    #print ("Number of hidden units:", len(hidden_states[layer_i][batch_i][token_i]))

    token_i = 5
    layer_i = 5
    vec = hidden_states[layer_i][batch_i][token_i]

    # Plot the values as a histogram to show their distribution.
    # plt.figure(figsize=(10,10))
    # plt.hist(vec, bins=200)
    # plt.show()

    # `hidden_states` is a Python list.
    #print('      Type of hidden_states: ', type(hidden_states))

    # Each layer in the list is a torch tensor.
    #print('Tensor shape for each layer: ', hidden_states[0].size())

    token_embeddings = torch.stack(hidden_states, dim=0)

    #token_embeddings.size()

    token_embeddings = torch.squeeze(token_embeddings, dim=1)

    #token_embeddings.size()

    token_embeddings = token_embeddings.permute(1,0,2)

    #token_embeddings.size()

    # Stores the token vectors, with shape [22 x 3,072]
    token_vecs_cat = []

    # `token_embeddings` is a [22 x 12 x 768] tensor.

    # For each token in the sentence...
    for token in token_embeddings:
        
        # `token` is a [12 x 768] tensor

        # Concatenate the vectors (that is, append them together) from the last 
        # four layers.
        # Each layer vector is 768 values, so `cat_vec` is length 3,072.
        cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)
        
        # Use `cat_vec` to represent `token`.
        token_vecs_cat.append(cat_vec)

    #print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))

    # Stores the token vectors, with shape [22 x 768]
    token_vecs_sum = []

    # `token_embeddings` is a [22 x 12 x 768] tensor.

    # For each token in the sentence...
    for token in token_embeddings:

        # `token` is a [12 x 768] tensor

        # Sum the vectors from the last four layers.
        sum_vec = torch.sum(token[-4:], dim=0)
        
        # Use `sum_vec` to represent `token`.
        token_vecs_sum.append(sum_vec)
    return token_vecs_sum, tokenized_text

def getPositionToWord(text):
  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
  tokenized_text = tokenizer.tokenize(text)

  

  words = list()
  for token in tokenized_text:
    if len(token) >= 2 and token[:2] == '##':
      words[-1].append(token)
    else:
      words.append(list())
      words[-1].append(token)

  word_position = list()
  start = 0

  for word in words:
    word_position.append(text.find(word[0], start))
    for word_part in word:
      start = text.find(word_part, start) + len(word_part)

  position_to_token_index = {word_position[i]: i for i in range(len(word_position))}



  print(tokens)
# print(len(tokens))
print(token_vecs_sum[0])
print(len(token_vecs_sum[0]))
# print(len(token_vecs_sum))
  return position_to_token_index