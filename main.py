# -*- coding: utf-8 -*-
"""NLP 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S9ihiz4uSkc3ib941IBcHLLNQ5qewgTB
"""

from google.colab import drive
drive.mount('/content/drive')
!cp -r /content/drive/MyDrive/processed .

!pip install transformers

# Commented out IPython magic to ensure Python compatibility.
import torch
from transformers import BertTokenizer, BertModel

# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows
import logging
#logging.basicConfig(level=logging.INFO)

import matplotlib.pyplot as plt
# % matplotlib inline

def get_embedding(text):
    # Load pre-trained model tokenizer (vocabulary)
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    # text = "Here is the sentence I want embeddings for."
    # marked_text = "[CLS] " + text + " [SEP]"

    # # Tokenize our sentence with the BERT tokenizer.
    # tokenized_text = tokenizer.tokenize(marked_text)

    # # Print out the tokens.
    # print (tokenized_text)

    # list(tokenizer.vocab.keys())[5000:5020]

    # Define a new example sentence with multiple meanings of the word "bank"
    #text = "After stealing money from the bank vault, the bank robber was seen " \
           #"fishing on the Mississippi river bank."

    # Add the special tokens.
    marked_text = "[CLS] " + text + " [SEP]"

    # Split the sentence into tokens.
    tokenized_text = tokenizer.tokenize(marked_text)

    # Map the token strings to their vocabulary indeces.
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
    # for tup in zip(tokenized_text, indexed_tokens):
    #   print('{:<12} {:>6,}'.format(tup[0], tup[1]))
    # # Display the words with their indeces.
    # for tup in zip(tokenized_text, indexed_tokens):
    #     print('{:<12} {:>6,}'.format(tup[0], tup[1]))

    segments_ids = [1] * len(tokenized_text)

    #print (segments_ids)

    # Convert inputs to PyTorch tensors
    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])

    # Load pre-trained model (weights)
    model = BertModel.from_pretrained('bert-base-uncased',
                                      output_hidden_states = True, # Whether the model returns all hidden-states.
                                      )

    # Put the model in "evaluation" mode, meaning feed-forward operation.
    model.eval()

    with torch.no_grad():

        outputs = model(tokens_tensor, segments_tensors)

        # Evaluating the model will return a different number of objects based on 
        # how it's  configured in the `from_pretrained` call earlier. In this case, 
        # becase we set `output_hidden_states = True`, the third item will be the 
        # hidden states from all layers. See the documentation for more details:
        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel
        hidden_states = outputs[2]

    #print ("Number of layers:", len(hidden_states), "  (initial embeddings + 12 BERT layers)")
    layer_i = 0

    #print ("Number of batches:", len(hidden_states[layer_i]))
    batch_i = 0

    #print ("Number of tokens:", len(hidden_states[layer_i][batch_i]))
    token_i = 0

    #print ("Number of hidden units:", len(hidden_states[layer_i][batch_i][token_i]))

    token_i = 5
    layer_i = 5
    vec = hidden_states[layer_i][batch_i][token_i]

    # Plot the values as a histogram to show their distribution.
    # plt.figure(figsize=(10,10))
    # plt.hist(vec, bins=200)
    # plt.show()

    # `hidden_states` is a Python list.
    #print('      Type of hidden_states: ', type(hidden_states))

    # Each layer in the list is a torch tensor.
    #print('Tensor shape for each layer: ', hidden_states[0].size())

    token_embeddings = torch.stack(hidden_states, dim=0)

    #token_embeddings.size()

    token_embeddings = torch.squeeze(token_embeddings, dim=1)

    #token_embeddings.size()

    token_embeddings = token_embeddings.permute(1,0,2)

    #token_embeddings.size()

    # Stores the token vectors, with shape [22 x 3,072]
    token_vecs_cat = []

    # `token_embeddings` is a [22 x 12 x 768] tensor.

    # For each token in the sentence...
    for token in token_embeddings:
        
        # `token` is a [12 x 768] tensor

        # Concatenate the vectors (that is, append them together) from the last 
        # four layers.
        # Each layer vector is 768 values, so `cat_vec` is length 3,072.
        cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)
        
        # Use `cat_vec` to represent `token`.
        token_vecs_cat.append(cat_vec)

    #print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))

    # Stores the token vectors, with shape [22 x 768]
    token_vecs_sum = []

    # `token_embeddings` is a [22 x 12 x 768] tensor.

    # For each token in the sentence...
    for token in token_embeddings:

        # `token` is a [12 x 768] tensor

        # Sum the vectors from the last four layers.
        sum_vec = torch.sum(token[-4:], dim=0)
        
        # Use `sum_vec` to represent `token`.
        token_vecs_sum.append(sum_vec)
    return token_vecs_sum, tokenized_text

#print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))

token_vecs_sum, tokens = get_embedding("my name is nikhil popli")

rel_dict = {}
rel_dict['NoRel'] = 0
rel_dict['Coref'] = 1
rel_dict['SuperSub'] = 2
rel_dict['SubSuper'] = 3

relations_list = []
import csv
def read_file_from_name(file_name):
  text = ''
  event_trigger_dict = {}
  with open(file_name) as csv_file:
      csv_reader = csv.reader(csv_file, delimiter='\t')
      line_count = 0
      for row in csv_reader:
          if line_count == 0:
              #print(row)
              text = row[1]
              line_count += 1
          elif row[0]=='Event':
              #print(row)
              event_trigger_dict[int(row[1])] = (row[2],row[4]) 
              line_count += 1
          else:
              relations_list.append((int(row[1]),int(row[2]),rel_dict[row[3]]))
              #print(row)
              line_count+=1
      print(line_count)
      return text,relations_list, event_trigger_dict





def getPositionToWord(text):
  # text = text.lower()
  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
  tokenized_text = tokenizer.tokenize(text)
  
  words = list()
  for token in tokenized_text:
    #print(token)
    if len(token) >= 2 and token[:2] == '##':
      words[-1].append(token)
    else:
      words.append(list())
      words[-1].append(token)

  word_position = list()
  start = 0

  for word in words:
    word_position.append(text.find(word[0], start))
    for word_part in word:
      if len(word_part) >= 2 and word_part[:2] == '##':
        word_part = word_part[2:]
      start = text.find(word_part, start) + len(word_part)

  position_to_token_index = {word_position[i]: i for i in range(len(word_position))}
  return position_to_token_index



text

pos_to_word

def get_emb(i1,i2,embeds):
  l1 = len(embeds[i1])
  vec = torch.zeros(2, 768)
  for i in range(l1):
    vec[0]+=embeds[i1][i]
  vec[0] = vec[0]/l1

  l2 = len(embeds[i2])
  for i in range(l2):
    vec[1]+=embeds[i2][i]
  vec[1] = vec[1]/l2
  vec = torch.flatten(vec)
  return vec

def get_the_data(marked_text, pos_to_word, embed_vec_sum, relations_list, event_trigger_dict):
  X = []
  Y = []

  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
  tokenized_text = tokenizer.tokenize(text)

  words = list()
  embeds = list()
  for i in range(len(tokenized_text)):
    token = tokenized_text[i]
    #print(token)
    if len(token) >= 2 and token[:2] == '##':
      words[-1].append(token)
      embeds[-1].append(embed_vec_sum[i])

    else:
      words.append(list())
      words[-1].append(token)

      embeds.append(list())
      embeds[-1].append(embed_vec_sum[i])
  for relation in relations_list:
    Y.append(relation[2])
    ev1 = relation[0]
    ev2 = relation[1]
    l1 = event_trigger_dict[ev1]
    l2 = event_trigger_dict[ev2]

    i1 = pos_to_word[int(l1[1])+6]
    i2 = pos_to_word[int(l2[1])+6]

    emb = get_emb(i1,i2,embeds)
    X.append(emb)
  return X,Y

train_data_X = []
train_data_Y = []
from os import listdir
from os.path import isfile, join
onlyfiles = [f for f in listdir('/content/processed/') if isfile(join('/content/processed/', f))]
print(onlyfiles)
cnt=0
for fname in onlyfiles:
  text,relations_list, event_trigger_dict = read_file_from_name('/content/processed/'+fname)
  try:
    embed_vec_sum, tok = get_embedding(text)
  except:
    cnt+=1
    continue
  marked_text = "[CLS] " + text.lower() + " [SEP]"
  print(marked_text)
  pos_to_word = getPositionToWord(marked_text)
  print(pos_to_word)
  print(len(relations_list))
  try:
    t_X,t_Y =  get_the_data(marked_text,pos_to_word,embed_vec_sum,relations_list, event_trigger_dict)
  except:
    
    continue
  train_data_X.extend(t_X)
  train_data_Y.extend(t_Y)
print(cnt)

train_data_X

import torch
import numpy as np
import matplotlib.pyplot as plt

#from sklearn.datasets import load_breast_cancer

#data = #load_breast_cancer()

# write split here for label
X, Y = train_data_X, train_data_Y
X = torch.stack(X)
Y = torch.tensor(Y)


# print(data.feature_names)
# print(data.target_names)

"""let's preprocess, normalize and create the model"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)


from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test) 

print(X_test.shape)

class BinaryClassification(torch.nn.Module):
  def __init__(self, input_dimension):
    super().__init__()
    self.linear1 = torch.nn.Linear(input_dimension,100)
    self.linear2 = torch.nn.Linear(100, 4)
    self.soft = torch.nn.Softmax(dim=1)
    self.relu = torch.nn.ReLU()

  def forward(self, X):
      X = torch.tensor(X).float()
      return self.soft(self.linear2(self.relu(self.linear1(X))))
    

_, input_dimension = X_train.shape

model = BinaryClassification(input_dimension)

"""train the model"""

def configure_loss_function(): 
  return torch.nn.CrossEntropyLoss()

def configure_optimizer(model):
  return torch.optim.Adam(model.parameters())

def full_gd(model, criterion, optimizer, X_train, y_train, n_epochs=2000):
  train_losses = np.zeros(n_epochs)
  test_losses = np.zeros(n_epochs)

  for it in range(n_epochs): 
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    outputs_test = model(X_test)
    loss_test = criterion(outputs_test, y_test)

    train_losses[it] = loss.item()
    test_losses[it] = loss_test.item()

    if (it + 1) % 50 == 0:
      print(f'In this epoch {it+1}/{n_epochs}, Training loss: {loss.item():.4f}, Test loss: {loss_test.item():.4f}')

  return train_losses, test_losses

# X_train = torch.from_numpy(X_train.astype(np.float32))
# X_test = torch.from_numpy(X_test.astype(np.float32))
# y_train = torch.from_numpy(y_train.astype(np.float32)).reshape(-1, 1)
# y_test = torch.from_numpy(y_test.astype(np.float32)).reshape(-1, 1)

criterion = configure_loss_function()
optimizer = configure_optimizer(model)
train_losses, test_losses = full_gd(model, criterion, optimizer, X_train, y_train)

plt.plot(train_losses, label = 'train loss')
plt.plot(test_losses, label = 'test loss')
plt.legend()
plt.show()

"""evaluate model"""

with torch.no_grad():
  p_train = model(X_train)
  #p_train = (p_train.numpy() > 0)

  train_acc = np.mean(y_train.numpy() == p_train)

  p_test = model(X_test)
  #p_test = (p_test > 0)
  #p_label = torch.argmax(p_test,dim = 1)

  test_acc = np.mean(y_test.numpy() == p_test)

print(train_acc)
print(test_acc)



